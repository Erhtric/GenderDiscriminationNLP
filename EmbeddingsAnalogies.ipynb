{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/WenXiaowei/unbiasing_nlp_embeddings/blob/main/EmbeddingsAnalogies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as gloader\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model_type, embedding_dim):\n",
    "    \"\"\"\n",
    "    Loads a pre-trained word embedding model via gensim library.\n",
    "\n",
    "    :param model_type: name of the word embedding model to load.\n",
    "    :param embedding_dim: size of the embedding space to consider\n",
    "\n",
    "    :return\n",
    "        - pre-trained word embedding model (gensim KeyedVectors object)\n",
    "    \"\"\"\n",
    "    download_path = \"\"\n",
    "\n",
    "    # Find the correct embedding model name\n",
    "    if model_type.strip().lower() == 'word2vec':\n",
    "        download_path = \"word2vec-google-news-300\"\n",
    "\n",
    "    elif model_type.strip().lower() == 'glove':\n",
    "        download_path = \"glove-wiki-gigaword-{}\".format(embedding_dimension)\n",
    "    elif model_type.strip().lower() == 'fasttext':\n",
    "        download_path = \"fasttext-wiki-news-subwords-300\"\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported embedding model type! Available ones: word2vec, glove, fasttext\")\n",
    "    \n",
    "    # Check download\n",
    "    try:\n",
    "        emb_model = gloader.load(download_path)\n",
    "    except ValueError as e:\n",
    "        print(\"Invalid embedding model name! Check the embedding dimension:\")\n",
    "        print(\"Word2Vec: 300\")\n",
    "        print(\"Glove: 50, 100, 200, 300\")\n",
    "        raise e\n",
    "\n",
    "    return emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 400000\n"
     ]
    }
   ],
   "source": [
    "# Modify these variables as you wish!\n",
    "# Glove -> 50, 100, 200, 300\n",
    "# Word2Vec -> 300\n",
    "# Fasttext -> 300\n",
    "embedding_model_type = \"glove\"\n",
    "embedding_dimension = 200\n",
    "\n",
    "# Load the embedding model\n",
    "embedding = load_embeddings(embedding_model_type, embedding_dimension)\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(embedding.key_to_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploit relation between words\n",
    "$Paris:France :: Tokyo:x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peppe\\anaconda3\\envs\\nlp\\lib\\site-packages\\scipy\\spatial\\distance.py:630: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar:\n",
      "[('japan', 0.26387566328048706),\n",
      " ('france', 0.39477771520614624),\n",
      " ('spain', 0.45271486043930054),\n",
      " ('canada', 0.4603247046470642),\n",
      " ('slovakia', 0.47672826051712036),\n",
      " ('britain', 0.47828537225723267),\n",
      " ('germany', 0.47892600297927856),\n",
      " ('italy', 0.47972118854522705),\n",
      " ('korea', 0.4806209206581116),\n",
      " ('romania', 0.48913657665252686)]\n"
     ]
    }
   ],
   "source": [
    "# relation reasoning\n",
    "target_words = [\"paris\", \"france\", \"tokyo\"]\n",
    "\n",
    "relationship_1 = embedding[target_words[0]] - embedding[target_words[1]]\n",
    "relationship_2 = [embedding[target_words[2]] - embedding[word] for word in embedding.key_to_index.keys()]\n",
    "\n",
    "similarities = {i: distance.cosine(relationship_1, rel2) for i, rel2 in enumerate(relationship_2)}\n",
    "\n",
    "# sort similarities by value\n",
    "similarities = dict(sorted(similarities.items(), key=lambda x: x[1]))\n",
    "\n",
    "# remove the target words from the list\n",
    "similarities.pop(embedding.key_to_index[target_words[2]], None)\n",
    "\n",
    "print(\"Most similar:\") \n",
    "pprint([(embedding.index_to_key[i], value) for i, value in list(similarities.items())[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploit the most similar vectors to some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar:\n",
      "[('whore', 0),\n",
      " ('slut', 0.39468371868133545),\n",
      " ('bitch', 0.44846659898757935),\n",
      " ('hypocrite', 0.4558418393135071),\n",
      " ('whores', 0.492634654045105),\n",
      " ('fucking', 0.5071277618408203),\n",
      " ('junkie', 0.5299773514270782),\n",
      " ('bastard', 0.5561682283878326),\n",
      " ('liar', 0.5697762668132782),\n",
      " ('pimp', 0.5821506381034851)]\n"
     ]
    }
   ],
   "source": [
    "# similarity reasoning\n",
    "query = embedding[\"whore\"]\n",
    "similarities = {i: distance.cosine(query, embedding[word]) for i, word in enumerate(embedding.key_to_index.keys())}\n",
    "\n",
    "# sort similarities by value\n",
    "similarities = dict(sorted(similarities.items(), key=lambda x: x[1]))\n",
    "\n",
    "print(\"Most similar:\") \n",
    "pprint([(embedding.index_to_key[i], value) for i, value in list(similarities.items())[:10]])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8218bdb2dad6187c4b0f3e84e26c53ed9d719227dd49e3deae672e3e8481b4c5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
